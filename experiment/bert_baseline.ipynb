{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import datetime\n",
    "import platform\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from transformers import BertConfig, BertModel, BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, AdamW\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, RandomSampler, SequentialSampler\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, \\\n",
    "                            roc_auc_score, confusion_matrix, classification_report, \\\n",
    "                            matthews_corrcoef, cohen_kappa_score, log_loss, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KoBERT : monologg/kobert\n",
    "- KR-BERT : snunlp/KR-BERT-char16424\n",
    "- KoELECTRA : monologg/koelectra-base-v3-discriminator\n",
    "    - input : token id, attention mask, token type id\n",
    "- Mental_BERT\n",
    "- Klue-RoBERTa : klue/roberta-base\n",
    "- KorBERT(후보)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맥북인 경우 mps 할당량 설정\n",
    "# import torch.mps\n",
    "# torch.mps.set_per_process_memory_fraction(0.8)\n",
    "\n",
    "# cache 정리 \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 셀 별로 러닝타임 측정\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델 딕셔너리\n",
    "bert_models = {'KoBERT' : 'monologg/kobert', 'KR-BERT' : 'snunlp/KR-BERT-char16424', 'KoELECTRA' : 'monologg/koelectra-base-v3-discriminator', 'Klue-RoBERTa' : 'klue/roberta-base'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "learning_rate =  5e-5\n",
    "epoch = 500\n",
    "\n",
    "# EarlyStopping 변수\n",
    "patience = 10\n",
    "early_stopping_epochs = 5\n",
    "best_loss = float('inf')\n",
    "\n",
    "MAX_LEN = 512\n",
    "seed_val = 42\n",
    "ep = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BERT 수행 위한 class\n",
    "@def : __init__\n",
    "      convert_data\n",
    "      convert_tensor\n",
    "      train\n",
    "'''\n",
    "class BertModel():\n",
    "\n",
    "    def __init__(self, MODEL_NAME) :\n",
    "        self.model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=11)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "        self.seed_val = seed_val\n",
    "        self.max_len = MAX_LEN\n",
    "        \n",
    "        # 디바이스 설정\n",
    "        os_name = platform.system()\n",
    "        if os_name == 'Darwin' :  # MacOS \n",
    "            self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "        elif os_name == 'Windows' :\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else :\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "    \n",
    "    '''\n",
    "    문장 전처리\n",
    "    @param series : tests, targets\n",
    "    @return list : token_ids_list, \n",
    "                   attention_mask, \n",
    "                   targets_list\n",
    "    '''\n",
    "    def convert_data(self, texts, targets):\n",
    "        token_ids_list, attention_mask, targets_list = [], [], []\n",
    "        for text, target in tqdm(zip(texts, targets), total=len(texts)):\n",
    "            tokens = []\n",
    "            sentences = text.split('.')\n",
    "            # 문장 구분 토큰 생성\n",
    "            sentences = ['[CLS]' + sentence + '[SEP]' for sentence in sentences]\n",
    "            # tokenize\n",
    "            tokens = [self.tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "            token_ids = [self.tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "\n",
    "            # Padding\n",
    "            input_ids = pad_sequences(token_ids, maxlen=self.max_len, dtype='long', truncating='post', padding='post')\n",
    "\n",
    "            # attention masking\n",
    "            attention_mask.append([[float(i>0) for i in seq] for seq in input_ids])\n",
    "            token_ids_list.append(input_ids)\n",
    "            targets_list.extend([target] * len(input_ids))\n",
    "        \n",
    "        return token_ids_list, attention_mask, targets_list\n",
    "    \n",
    "    '''\n",
    "    tensor로 변환\n",
    "    @param list : token_ids_list, attention_mask, target\n",
    "    @return tensor : tensor_data\n",
    "    '''\n",
    "    def convert_tensor(self, token_ids_list, attention_mask, target) :\n",
    "        token_ids_tensor = torch.tensor([np.array(item) for sublist in token_ids_list for item in sublist], dtype=torch.long)\n",
    "        attention_mask_tensor = torch.tensor([np.array(item) for sublist in attention_mask for item in sublist], dtype=torch.float)\n",
    "        targets_tensor = torch.tensor(np.array(target), dtype=torch.long)\n",
    "\n",
    "        # 사이즈 확인\n",
    "        print(\"Token IDs Tensor Size: \", token_ids_tensor.size())\n",
    "        print(\"Attention Mask Tensor Size: \", attention_mask_tensor.size())\n",
    "        print(\"Targets Tensor Size: \", targets_tensor.size())\n",
    "\n",
    "        tensor_data = TensorDataset(token_ids_tensor, attention_mask_tensor, targets_tensor)\n",
    "\n",
    "        return tensor_data\n",
    "    \n",
    "    '''\n",
    "    train/valid 수행\n",
    "    @param TensorDataset : train_loader, val_loader\n",
    "    @return \n",
    "    '''\n",
    "    def train(self, train_loader, val_loader) :\n",
    "        model = self.model\n",
    "        seed_val = self.seed_val\n",
    "        device = self.device\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=ep)\n",
    "        scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epoch)\n",
    "        \n",
    "        # seed 고정\n",
    "        random.seed(seed_val)\n",
    "        np.random.seed(seed_val)\n",
    "        torch.manual_seed(seed_val)\n",
    "        if self.device == 'cuda' :\n",
    "            torch.cuda.manual_seed(seed_val)\n",
    "        elif self.device == 'mps' :\n",
    "            torch.mps.manual_seed(seed_val)\n",
    "        \n",
    "        model.to(device)\n",
    "        model.zero_grad()\n",
    "        train_len = len(train_loader)\n",
    "\n",
    "        for e in tqdm(range(0, epoch)) :\n",
    "            model.train()\n",
    "            total_loss, total_accuracy = 0, 0\n",
    "            train_true, train_pred = [], []\n",
    "            print(f'Epoch : {e+1} in {epoch} >>>>>>>>>>>>>>>>> ')\n",
    "            \n",
    "            for step, batch in tqdm(enumerate(train_loader)):\n",
    "                batch = tuple(item.to(self.device) for item in batch)\n",
    "                batch_input_ids, batch_input_mask, batch_labels = batch\n",
    "                outputs = model(input_ids=batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "\n",
    "                train_true.extend(batch_labels.tolist())\n",
    "                train_pred.extend(np.argmax(outputs.logits.detach().cpu().numpy(), axis=1).tolist())\n",
    "            \n",
    "            avg_loss = total_loss / train_len\n",
    "            avg_accuracy = total_accuracy / train_len\n",
    "            print(f'Epoch {e+1} Average train loss : {avg_loss}    /   accuracy : {avg_accuracy}')\n",
    "\n",
    "            train_confusion = confusion_matrix(train_true, train_pred)\n",
    "            print('Train Confusion Matrix:\\n', train_confusion)\n",
    "\n",
    "            train_classification_report = classification_report(train_true, train_pred)\n",
    "            print('Train Classification Report:\\n', train_classification_report)\n",
    "\n",
    "        \n",
    "            print(f'Running Validation...........')\n",
    "\n",
    "            model.eval()\n",
    "            val_len = len(val_loader)\n",
    "            val_loss, val_accuracy = 0, 0\n",
    "            val_true, val_pred = [], []\n",
    "\n",
    "            for batch in val_loader :\n",
    "                batch_input_ids, batch_input_mask, batch_labels = [item.to(self.device) for item in batch]\n",
    "                with torch.no_grad() :\n",
    "                    outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "                    loss = outputs.loss\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    val_true.extend(batch_labels.tolist())\n",
    "                    val_pred.extend(np.argmax(outputs.logits.detach().cpu().numpy(), axis=1).tolist())    \n",
    "\n",
    "            eval_avg_loss = val_loss / val_len\n",
    "            eval_avg_accuracy = val_accuracy / val_len\n",
    "            print(f'Epoch {e+1} Average Validataion loss : {eval_avg_loss}  /   accuracy : {eval_avg_accuracy}')\n",
    "\n",
    "            val_confusion = confusion_matrix(val_true, val_pred)\n",
    "            print('Validation Confusion Matrix:\\n', val_confusion)\n",
    "\n",
    "            val_classification_report = classification_report(val_true, val_pred)\n",
    "            print('Validation Classification Report:\\n', val_classification_report)\n",
    "\n",
    "            # Early Stopping\n",
    "            if eval_avg_loss < best_loss:\n",
    "                best_loss = eval_avg_loss\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                \n",
    "            if early_stopping_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break     \n",
    "\n",
    "    '''\n",
    "    학습 모델 저장\n",
    "    @param  PATH 지정 필요\n",
    "    @return \n",
    "    '''\n",
    "    def model_save(self) :\n",
    "        PATH = './bert_model'\n",
    "        if not os.path.exists(PATH):\n",
    "            os.makedirs(PATH)\n",
    "        torch.save(self.model, f'{PATH}/{self.model}_model.pt')  # 모델 저장\n",
    "        torch.save(self.model.state_dict(), f'{PATH}/{self.model}_model_state_dict.pt')  # 모델 객체의 state_dict 저장\n",
    "        torch.save({\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict()\n",
    "        }, PATH + 'all.tar')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class name - Run\n",
    "# parameter - df : 수행할 데이터프레임, model_name : 사용할 모델명\n",
    "# 데이터 전처리부터 학습까지 수행\n",
    "class Run() :\n",
    "    def __init__(self, df, model_name) :\n",
    "        self.df = df\n",
    "        self.model_name = model_name\n",
    "    def run(self) :\n",
    "        # model 선언\n",
    "        bert_model = BertModel(self.model_name)\n",
    "        # train_test_split\n",
    "        train_x, val_x, train_y, val_y = train_test_split(self.df['HS'], self.df['label'], test_size=0.2, random_state=seed_val)\n",
    "        # 데이터 전처리\n",
    "        train_token_ids_list, train_attention_mask, train_targets = bert_model.convert_data(train_x, train_y)\n",
    "        val_token_ids_list, val_attention_mask, val_targets = bert_model.convert_data(val_x, val_y)\n",
    "        # tensor로 변환\n",
    "        train_data = bert_model.convert_tensor(train_token_ids_list, train_attention_mask, train_targets)\n",
    "        val_data = bert_model.convert_tensor(val_token_ids_list, val_attention_mask, val_targets)\n",
    "        # DataLoader\n",
    "        train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        # Train\n",
    "        bert_model.train(train_loader, val_loader)\n",
    "        # model save   PATH 지정 필요\n",
    "        bert_model.model_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어오기\n",
    "df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위해 수치형으로 변환\n",
    "df['label'] = df['label'].map({\n",
    "                                'ADHD' : 0,\n",
    "                                'PTSD(posttraumatic_stress_disorder)' : 1,\n",
    "                                'bipolar_disorder'                   : 2,\n",
    "                                'obsessive_compulsive_disorder'      : 3,\n",
    "                                'normal'                             : 4,\n",
    "                                'paranoid_personality_disorder'  : 5,\n",
    "                                'avoidant_personality_disorder'  : 6,\n",
    "                                'seperation_anxiety_disorder'    : 7,\n",
    "                                'MDD(major_depressive_disorder)' : 8,\n",
    "                                'generalized_anxiety_disorder'   : 9,\n",
    "                                'neurocognitive_disorders'       : 10\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미사용 컬럼 삭제\n",
    "df.drop('profile_persona_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "MODEL_NAME = bert_models['KR-BERT']\n",
    "# 데이터 전처리 및 학습을 위한 클래스 선언\n",
    "bert_run = Run(df, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process 수행\n",
    "bert_run.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import datetime\n",
    "import platform\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertConfig, BertModel, BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, AdamW\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, RandomSampler, SequentialSampler\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, \\\n",
    "                            roc_auc_score, confusion_matrix, classification_report, \\\n",
    "                            matthews_corrcoef, cohen_kappa_score, log_loss, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KoBERT : monologg/kobert\n",
    "- KR-BERT : snunlp/KR-BERT-char16424\n",
    "- KoELECTRA : monologg/koelectra-base-v3-discriminator\n",
    "    - input : token id, attention mask, token type id\n",
    "- Mental_BERT\n",
    "- Klue-RoBERTa : klue/roberta-base\n",
    "- KorBERT(후보)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맥북인 경우 mps 할당량 설정\n",
    "# import torch.mps\n",
    "# torch.mps.set_per_process_memory_fraction(0.8)\n",
    "\n",
    "# cache 정리 \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델 딕셔너리\n",
    "bert_models = {'KoBERT' : 'monologg/kobert', 'KR-BERT' : 'snunlp/KR-BERT-char16424', 'KoELECTRA' : 'monologg/koelectra-base-v3-discriminator', 'Klue-RoBERTa' : 'klue/roberta-base'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "learning_rate =  5e-5\n",
    "epoch = 500\n",
    "\n",
    "# EarlyStopping 변수\n",
    "patience = 10\n",
    "early_stopping_epochs = 5\n",
    "best_loss = float('inf')\n",
    "\n",
    "MAX_LEN = 512\n",
    "seed_val = 42\n",
    "ep = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel():\n",
    "\n",
    "    def __init__(self, MODEL_NAME) :\n",
    "        self.model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=11)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "        self.seed_val = seed_val\n",
    "        self.max_len = MAX_LEN\n",
    "        \n",
    "        # 디바이스 설정\n",
    "        os_name = platform.system()\n",
    "        if os_name == 'Darwin' :  # MacOS \n",
    "            self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "        elif os_name == 'Windows' :\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else :\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "    # 문장 전처리\n",
    "    def convert_data(self, texts, targets):\n",
    "        token_ids_list, attention_mask, targets_list = [], [], []\n",
    "        for text, target in tqdm(zip(texts, targets), total=len(texts)):\n",
    "            tokens = []\n",
    "            sentences = text.split('.')\n",
    "            # 문장 구분 토큰 생성\n",
    "            sentences = ['[CLS]' + sentence + '[SEP]' for sentence in sentences if sentence]\n",
    "            # tokenize\n",
    "            tokens = [self.tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "            token_ids = [self.tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "\n",
    "            # Padding\n",
    "            input_ids = pad_sequences(token_ids, maxlen=self.max_len, dtype='long', truncating='post', padding='post')\n",
    "\n",
    "            # attention masking\n",
    "            attention_mask.append([[float(i>0) for i in seq] for seq in input_ids])\n",
    "            token_ids_list.append(input_ids)\n",
    "            targets_list.extend([target] * len(input_ids))\n",
    "        \n",
    "        return token_ids_list, attention_mask, targets_list\n",
    "    \n",
    "    # 학습을 위한 tensor로 변환\n",
    "    def convert_tensor(self, token_ids_list, attention_mask, target) :\n",
    "        token_ids_tensor = torch.tensor([item for sublist in token_ids_list for item in sublist], dtype=torch.long)\n",
    "        attention_mask_tensor = torch.tensor([item for sublist in attention_mask for item in sublist], dtype=torch.float)\n",
    "        targets_tensor = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "        # 사이즈 확인\n",
    "        print(\"Token IDs Tensor Size: \", token_ids_tensor.size())\n",
    "        print(\"Attention Mask Tensor Size: \", attention_mask_tensor.size())\n",
    "        print(\"Targets Tensor Size: \", targets_tensor.size())\n",
    "\n",
    "        tensor_data = TensorDataset(token_ids_tensor, attention_mask_tensor, targets_tensor)\n",
    "\n",
    "        return tensor_data\n",
    "    \n",
    "    # Train\n",
    "    def train(self, train_loader, val_loader) :\n",
    "        model = self.model\n",
    "        seed_val = self.seed_val\n",
    "        device = self.device\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=ep)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epoch)\n",
    "        \n",
    "        # seed 고정\n",
    "        random.seed(seed_val)\n",
    "        np.random.seed(seed_val)\n",
    "        torch.manual_seed(seed_val)\n",
    "        if self.device == 'cuda' :\n",
    "            torch.cuda.manual_seed(seed_val)\n",
    "        elif self.device == 'mps' :\n",
    "            torch.mps.manual_seed(seed_val)\n",
    "        \n",
    "        model.to(device)\n",
    "        model.zero_grad()\n",
    "        train_len = len(train_loader)\n",
    "\n",
    "        for e in tqdm(range(0, epoch)) :\n",
    "            model.train()\n",
    "            total_loss, total_accuracy = 0, 0\n",
    "            train_true, train_pred = [], []\n",
    "            print(f'Epoch : {e+1} in {epoch} >>>>>>>>>>>>>>>>> ')\n",
    "            \n",
    "            for step, batch in tqdm(enumerate(train_loader)):\n",
    "                batch = tuple(item.to(self.device) for item in batch)\n",
    "                batch_input_ids, batch_input_mask, batch_labels = batch\n",
    "                outputs = model(input_ids=batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "\n",
    "                train_true.extend(batch_labels.tolist())\n",
    "                train_pred.extend(np.argmax(outputs.logits.detach().cpu().numpy(), axis=1).tolist())\n",
    "            \n",
    "            avg_loss = total_loss / train_len\n",
    "            avg_accuracy = total_accuracy / train_len\n",
    "            print(f'Epoch {e+1} Average train loss : {avg_loss}    /   accuracy : {avg_accuracy}')\n",
    "\n",
    "            train_confusion = confusion_matrix(train_true, train_pred)\n",
    "            print('Train Confusion Matrix:\\n', train_confusion)\n",
    "\n",
    "            train_classification_report = classification_report(train_true, train_pred)\n",
    "            print('Train Classification Report:\\n', train_classification_report)\n",
    "\n",
    "        \n",
    "            print(f'Running Validation...........')\n",
    "\n",
    "            model.eval()\n",
    "            val_len = len(val_loader)\n",
    "            val_loss, val_accuracy = 0, 0\n",
    "            val_true, val_pred = [], []\n",
    "\n",
    "            for batch in val_loader :\n",
    "                batch_input_ids, batch_input_mask, batch_labels = [item.to(self.device) for item in batch]\n",
    "                with torch.no_grad() :\n",
    "                    outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "                    loss = outputs.loss\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    val_true.extend(batch_labels.tolist())\n",
    "                    val_pred.extend(np.argmax(outputs.logits.detach().cpu().numpy(), axis=1).tolist())    \n",
    "\n",
    "            eval_avg_loss = val_loss / val_len\n",
    "            eval_avg_accuracy = val_accuracy / val_len\n",
    "            print(f'Epoch {e+1} Average Validataion loss : {eval_avg_loss}  /   accuracy : {eval_avg_accuracy}')\n",
    "\n",
    "            val_confusion = confusion_matrix(val_true, val_pred)\n",
    "            print('Validation Confusion Matrix:\\n', val_confusion)\n",
    "\n",
    "            val_classification_report = classification_report(val_true, val_pred)\n",
    "            print('Validation Classification Report:\\n', val_classification_report)\n",
    "\n",
    "            # Early Stopping\n",
    "            if eval_avg_loss < best_loss:\n",
    "                best_loss = eval_avg_loss\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                \n",
    "            if early_stopping_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class name - Run\n",
    "# parameter - df : 수행할 데이터프레임, model_name : 사용할 모델명\n",
    "# 데이터 전처리부터 학습까지 수행\n",
    "class Run() :\n",
    "    def __init__(self, df, model_name) :\n",
    "        self.df = df\n",
    "        self.model_name = model_name\n",
    "    def run(self) :\n",
    "        # model 선언\n",
    "        bert_model = BertModel(self.model_name)\n",
    "        # train_test_split\n",
    "        train_x, val_x, train_y, val_y = train_test_split(self.df['HS'], self.df['label'], test_size=0.2, random_state=seed_val)\n",
    "        # 데이터 전처리\n",
    "        train_token_ids_list, train_attention_mask, train_targets = bert_model.convert_data(train_x, train_y)\n",
    "        val_token_ids_list, val_attention_mask, val_targets = bert_model.convert_data(val_x, val_y)\n",
    "        # tensor로 변환\n",
    "        train_data = bert_model.convert_tensor(train_token_ids_list, train_attention_mask, train_targets)\n",
    "        val_data = bert_model.convert_tensor(val_token_ids_list, val_attention_mask, val_targets)\n",
    "        # DataLoader\n",
    "        train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        # Train\n",
    "        bert_model.train(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어오기\n",
    "df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위해 수치형으로 변환\n",
    "df['label'] = df['label'].map({\n",
    "                                'ADHD' : 0,\n",
    "                                'PTSD(posttraumatic_stress_disorder)' : 1,\n",
    "                                'bipolar_disorder'                   : 2,\n",
    "                                'obsessive_compulsive_disorder'      : 3,\n",
    "                                'normal'                             : 4,\n",
    "                                'paranoid_personality_disorder'  : 5,\n",
    "                                'avoidant_personality_disorder'  : 6,\n",
    "                                'seperation_anxiety_disorder'    : 7,\n",
    "                                'MDD(major_depressive_disorder)' : 8,\n",
    "                                'generalized_anxiety_disorder'   : 9,\n",
    "                                'neurocognitive_disorders'       : 10\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미사용 컬럼 삭제\n",
    "df.drop('profile_persona_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "MODEL_NAME = bert_models['KR-BERT']\n",
    "# 데이터 전처리 및 학습을 위한 클래스 선언\n",
    "bert_run = Run(df, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gimsan/.pyenv/versions/3.9.16/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-BERT-char16424 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 41302/41302 [00:27<00:00, 1516.99it/s]\n",
      "100%|██████████| 10326/10326 [00:07<00:00, 1328.22it/s]\n",
      "/var/folders/vh/cy3znnkx7y970ygsc4r_tyzw0000gn/T/ipykernel_57679/2462329695.py:39: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  token_ids_tensor = torch.tensor([item for sublist in token_ids_list for item in sublist], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start convert_tensor \n",
      "Token IDs Tensor Size:  torch.Size([151160, 512])\n",
      "Attention Mask Tensor Size:  torch.Size([151160, 512])\n",
      "Targets Tensor Size:  torch.Size([151160])\n",
      "Start convert_tensor \n",
      "Token IDs Tensor Size:  torch.Size([37684, 512])\n",
      "Attention Mask Tensor Size:  torch.Size([37684, 512])\n",
      "Targets Tensor Size:  torch.Size([37684])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 in 500 / Step : 0\n",
      "Epoch : 1 in 500 / Step : 500\n",
      "Epoch : 1 in 500 / Step : 1000\n",
      "Epoch : 1 in 500 / Step : 1500\n",
      "Epoch : 1 in 500 / Step : 2000\n",
      "Epoch : 1 in 500 / Step : 2500\n",
      "Epoch : 1 in 500 / Step : 3000\n",
      "Epoch : 1 in 500 / Step : 3500\n",
      "Epoch : 1 in 500 / Step : 4000\n",
      "Epoch : 1 in 500 / Step : 4500\n",
      "Epoch : 1 in 500 / Step : 5000\n",
      "Epoch : 1 in 500 / Step : 5500\n",
      "Epoch : 1 in 500 / Step : 6000\n",
      "Epoch : 1 in 500 / Step : 6500\n",
      "Epoch : 1 in 500 / Step : 7000\n",
      "Epoch : 1 in 500 / Step : 7500\n",
      "Epoch : 1 in 500 / Step : 8000\n",
      "Epoch : 1 in 500 / Step : 8500\n",
      "Epoch : 1 in 500 / Step : 9000\n",
      "Epoch : 1 in 500 / Step : 9500\n",
      "Epoch : 1 in 500 / Step : 10000\n"
     ]
    }
   ],
   "source": [
    "# process 수행\n",
    "bert_run.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

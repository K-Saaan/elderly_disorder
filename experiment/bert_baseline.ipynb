{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import datetime\n",
    "import platform\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from transformers import BertConfig, BertModel, BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, AdamW\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, RandomSampler, SequentialSampler\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, \\\n",
    "                            roc_auc_score, confusion_matrix, classification_report, \\\n",
    "                            matthews_corrcoef, cohen_kappa_score, log_loss, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KoBERT : monologg/kobert\n",
    "- KR-BERT : snunlp/KR-BERT-char16424\n",
    "- KoELECTRA : monologg/koelectra-base-v3-discriminator\n",
    "    - input : token id, attention mask, token type id\n",
    "- Mental_BERT : https://huggingface.co/AIMH/mental-bert-base-cased\n",
    "- Klue-RoBERTa : klue/roberta-base\n",
    "- KorBERT(후보)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipython-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맥북인 경우 mps 할당량 설정\n",
    "# import torch.mps\n",
    "# torch.mps.set_per_process_memory_fraction(0.8)\n",
    "\n",
    "# cache 정리 \n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# 셀 별로 러닝타임 측정\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델 딕셔너리\n",
    "bert_models = {'KoBERT' : 'monologg/kobert', \n",
    "               'KR-BERT' : 'snunlp/KR-BERT-char16424', \n",
    "               'KoELECTRA' : 'monologg/koelectra-base-v3-discriminator', \n",
    "               'Klue-RoBERTa' : 'klue/roberta-base', \n",
    "               'Mental_BERT' : 'AIMH/mental-bert-base-cased'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 4\n",
    "BATCH_SIZE = 32\n",
    "learning_rate =  5e-5\n",
    "# epoch = 500\n",
    "epoch = 10\n",
    "\n",
    "# EarlyStopping 변수\n",
    "patience = 3\n",
    "# early_stopping_epochs = 5 # 이거 뭐지\n",
    "# best_loss = float('inf')\n",
    "\n",
    "MAX_LEN = 512\n",
    "seed_val = 42\n",
    "ep = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BERT 수행 위한 class\n",
    "@def : __init__\n",
    "      convert_data\n",
    "      convert_tensor\n",
    "      train\n",
    "'''\n",
    "class BertModel():\n",
    "\n",
    "    def __init__(self, MODEL_NAME, TOKEN=None) :\n",
    "        self.model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=11, token=TOKEN)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, token=TOKEN)\n",
    "        self.seed_val = seed_val\n",
    "        self.max_len = MAX_LEN\n",
    "        self.best_loss = float('inf')  # 초기화\n",
    "        \n",
    "        # 디바이스 설정\n",
    "        os_name = platform.system()\n",
    "        if os_name == 'Darwin' :  # MacOS \n",
    "            self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "        elif os_name == 'Windows' :\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else :\n",
    "            # self.device = torch.device('cpu')\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    '''\n",
    "    문장 전처리\n",
    "    @param series : tests, targets\n",
    "    @return list : token_ids_list, \n",
    "                   attention_mask, \n",
    "                   targets_list\n",
    "\n",
    "    `240508 기존: 각 문장마다 개별적으로 토큰화하고 패딩함 => 변경: 모든 문장을 처리한 후, 전체 데이터에 대해 한 번에 패딩 적용\n",
    "\n",
    "    '''\n",
    "    # def convert_data(self, texts, targets):\n",
    "    #     token_ids_list, attention_mask, targets_list = [], [], []\n",
    "    #     for text, target in tqdm(zip(texts, targets), total=len(texts)):\n",
    "    #         tokens = []\n",
    "    #         sentences = text.split('.')\n",
    "    #         # 문장 구분 토큰 생성\n",
    "    #         sentences = ['[CLS]' + sentence + '[SEP]' for sentence in sentences]\n",
    "    #         # tokenize\n",
    "    #         tokens = [self.tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "    #         token_ids = [self.tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "\n",
    "    #         # Padding\n",
    "    #         input_ids = pad_sequences(token_ids, maxlen=self.max_len, dtype='long', truncating='post', padding='post')\n",
    "\n",
    "    #         # attention masking\n",
    "    #         attention_mask.append([[float(i>0) for i in seq] for seq in input_ids])\n",
    "    #         token_ids_list.append(input_ids)\n",
    "    #         targets_list.extend([target] * len(input_ids))\n",
    "        \n",
    "    #     return token_ids_list, attention_mask, targets_list\n",
    "    def convert_data(self, texts, targets):\n",
    "        token_ids_list, attention_masks, targets_list = [], [], []\n",
    "        \n",
    "        for text, target in tqdm(zip(texts, targets), total=len(texts)):\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,  # 문장 시작과 끝에 특수 토큰 추가\n",
    "                max_length=self.max_len,  # 시퀀스 최대 길이 설정\n",
    "                pad_to_max_length=True,   # 패딩 적용\n",
    "                return_attention_mask=True,  # 어텐션 마스크 생성\n",
    "                return_tensors='pt',     # 파이토치 텐서로 반환\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            token_ids_list.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            targets_list.append(target)\n",
    "        \n",
    "        token_ids_tensor = torch.cat(token_ids_list, dim=0)\n",
    "        attention_mask_tensor = torch.cat(attention_masks, dim=0)\n",
    "        targets_tensor = torch.tensor(targets_list, dtype=torch.long)\n",
    "        \n",
    "        return token_ids_tensor, attention_mask_tensor, targets_tensor\n",
    "\n",
    "    \n",
    "    '''\n",
    "    tensor로 변환\n",
    "    @param list : token_ids_list, attention_mask, target\n",
    "    @return tensor : tensor_data\n",
    "\n",
    "    `240508 김윤겸- convert_data 함수에서 반환된 token_ids_list와 attention_masks는 이미 텐서 객체로 반환되고 있음. 따라서 convert_tensor 삭제 처리\n",
    "    '''\n",
    "    def convert_tensor(self, token_ids_list, attention_mask, target) :\n",
    "        token_ids_tensor = torch.tensor([np.array(item) for sublist in token_ids_list for item in sublist], dtype=torch.long)\n",
    "        attention_mask_tensor = torch.tensor([np.array(item) for sublist in attention_mask for item in sublist], dtype=torch.float)\n",
    "        targets_tensor = torch.tensor(np.array(target), dtype=torch.long)\n",
    "\n",
    "        # 사이즈 확인\n",
    "        print(\"Token IDs Tensor Size: \", token_ids_tensor.size())\n",
    "        print(\"Attention Mask Tensor Size: \", attention_mask_tensor.size())\n",
    "        print(\"Targets Tensor Size: \", targets_tensor.size())\n",
    "\n",
    "        tensor_data = TensorDataset(token_ids_tensor, attention_mask_tensor, targets_tensor)\n",
    "\n",
    "        return tensor_data\n",
    "    \n",
    "    '''\n",
    "    train/valid 수행\n",
    "    @param TensorDataset : train_loader, val_loader\n",
    "    @return \n",
    "    '''\n",
    "    def train(self, train_loader, val_loader):\n",
    "        model = self.model\n",
    "        seed_val = self.seed_val\n",
    "        device = self.device\n",
    "        print(\"현재 device 정보:\", device)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=ep)\n",
    "        scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epoch)\n",
    "\n",
    "        random.seed(seed_val)\n",
    "        np.random.seed(seed_val)\n",
    "        torch.manual_seed(seed_val)\n",
    "        torch.cuda.manual_seed_all(seed_val)  # CUDA seed 고정\n",
    "\n",
    "        model.to(device)\n",
    "        model.zero_grad()\n",
    "\n",
    "        for e in range(epoch): # batch 단위에서 이뤄지도록 여기선 삭제\n",
    "            model.train()\n",
    "            total_loss, total_correct, total_samples = 0, 0, 0 # 정확도 개선\n",
    "\n",
    "            for step, batch in tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {e+1}/{epoch}', leave=False):\n",
    "                batch = tuple(item.to(device) for item in batch)\n",
    "                batch_input_ids, batch_input_mask, batch_labels = batch\n",
    "\n",
    "                model.zero_grad()\n",
    "                outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                logits = outputs.logits.detach()\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                total_correct += (predictions == batch_labels).sum().item()\n",
    "                total_samples += batch_labels.size(0)\n",
    "\n",
    "            # 정확도 개선\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            train_accuracy = total_correct / total_samples\n",
    "            print(f'Epoch {e+1}: Average train loss: {avg_train_loss:.4f} / Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss, val_correct, val_samples = 0, 0, 0\n",
    "            val_true, val_pred = [], []  # 초기화\n",
    "\n",
    "            for batch in val_loader:\n",
    "                batch = tuple(item.to(device) for item in batch)\n",
    "                batch_input_ids, batch_input_mask, batch_labels = batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "                    loss = outputs.loss\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    logits = outputs.logits.detach()\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "                    val_correct += (predictions == batch_labels).sum().item()\n",
    "                    val_samples += batch_labels.size(0)\n",
    "                    val_true.extend(batch_labels.cpu().numpy())\n",
    "                    val_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = val_correct / val_samples\n",
    "            print(f'Epoch {e+1}: Average validation loss: {avg_val_loss:.4f} / Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "            if e == epoch - 1:  # 마지막 에포크에서만 classification report 출력\n",
    "                print('Final Validation Classification Report:\\n', classification_report(val_true, val_pred))\n",
    "\n",
    "            # Early Stopping\n",
    "            if avg_val_loss < self.best_loss:\n",
    "                self.best_loss = avg_val_loss\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                \n",
    "            if early_stopping_counter >= patience:\n",
    "                print(\">>> Early stopping triggered!\")\n",
    "                print('Final Validation Classification Report:\\n', classification_report(val_true, val_pred))\n",
    "                break\n",
    "\n",
    "\n",
    "    '''\n",
    "    학습 모델 저장\n",
    "    @param  PATH 지정 필요\n",
    "    @return \n",
    "\n",
    "    `240508 윤겸: 파일 이름을 지정할 때 모델 객체의 문자열 표현을 사용하고 있기 때문에 파일 이름이 지나치게 길어져 시스템의 파일 이름 길이 제한을 초과 => 변경\n",
    "    '''\n",
    "    def model_save(self, curr_model):\n",
    "        PATH = f'./{curr_model}'\n",
    "        if not os.path.exists(PATH):\n",
    "            os.makedirs(PATH)\n",
    "\n",
    "        # 모델과 상태 사전을 저장합니다. 파일 이름을 'bert_model.pt'와 'bert_model_state_dict.pt'로 고정합니다.\n",
    "        model_file_path = os.path.join(PATH, 'bert_model.pt')\n",
    "        state_dict_path = os.path.join(PATH, 'bert_model_state_dict.pt')\n",
    "        all_data_path = os.path.join(PATH, 'all.tar')\n",
    "\n",
    "        torch.save(self.model, model_file_path)  # 모델 전체 저장\n",
    "        torch.save(self.model.state_dict(), state_dict_path)  # 모델의 상태 사전(state_dict) 저장\n",
    "\n",
    "        # 옵티마이저 상태 포함 전체 데이터 저장\n",
    "        torch.save({\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict()\n",
    "        }, all_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class name - Run\n",
    "# parameter - df : 수행할 데이터프레임, model_name : 사용할 모델명\n",
    "# 데이터 전처리부터 학습까지 수행\n",
    "class Run() :\n",
    "    def __init__(self, df, model_name, token, curr_model) :\n",
    "        self.df = df\n",
    "        self.model_name = model_name\n",
    "        self.token = token\n",
    "        self.curr_model = curr_model\n",
    "\n",
    "    def run(self) :\n",
    "        print(f'{self.curr_model} Start >>>>>>>>>> ')\n",
    "        # model 선언\n",
    "        bert_model = BertModel(self.model_name)\n",
    "\n",
    "        # train_test_split\n",
    "        train_x, val_x, train_y, val_y = train_test_split(self.df['HS'], self.df['label'], test_size=0.2, random_state=seed_val)\n",
    "\n",
    "        # 데이터 전처리\n",
    "        train_token_ids_list, train_attention_mask, train_targets = bert_model.convert_data(train_x, train_y)\n",
    "        val_token_ids_list, val_attention_mask, val_targets = bert_model.convert_data(val_x, val_y)\n",
    "\n",
    "        # # tensor로 변환\n",
    "        # train_data = bert_model.convert_tensor(train_token_ids_list, train_attention_mask, train_targets)\n",
    "        # val_data = bert_model.convert_tensor(val_token_ids_list, val_attention_mask, val_targets)\n",
    "\n",
    "        # # DataLoader\n",
    "        # train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        # val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        # DataLoader 생성\n",
    "        train_data = TensorDataset(train_token_ids_list, train_attention_mask, torch.tensor(train_targets, dtype=torch.long))\n",
    "        val_data = TensorDataset(val_token_ids_list, val_attention_mask, torch.tensor(val_targets, dtype=torch.long))\n",
    "        train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # Train\n",
    "        bert_model.train(train_loader, val_loader)\n",
    "\n",
    "        # model save   PATH 지정 필요\n",
    "        bert_model.model_save(self.curr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어오기\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# 학습을 위해 수치형으로 변환\n",
    "df['label'] = df['label'].map({\n",
    "                                'ADHD' : 0,\n",
    "                                'PTSD(posttraumatic_stress_disorder)' : 1,\n",
    "                                'bipolar_disorder'                   : 2,\n",
    "                                'obsessive_compulsive_disorder'      : 3,\n",
    "                                'normal'                             : 4,\n",
    "                                'paranoid_personality_disorder'  : 5,\n",
    "                                'avoidant_personality_disorder'  : 6,\n",
    "                                'seperation_anxiety_disorder'    : 7,\n",
    "                                'MDD(major_depressive_disorder)' : 8,\n",
    "                                'generalized_anxiety_disorder'   : 9,\n",
    "                                'neurocognitive_disorders'       : 10\n",
    "                            })\n",
    "\n",
    "# 미사용 컬럼 삭제\n",
    "df.drop('profile_persona_id', axis=1, inplace=True)\n",
    "\n",
    "# 데이터셋의 일부만 사용 (예: 10% 샘플링)\n",
    "df_sampled = df.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "MODEL_NAME = bert_models['Mental_BERT']\n",
    "# 저장을 위한 현재 모델명 파라미터로 전단\n",
    "curr_model = 'Mental_BERT'\n",
    "# huggingface token\n",
    "TOKEN = \"\"\n",
    "# 데이터 전처리 및 학습을 위한 클래스 선언\n",
    "bert_run = Run(df_sampled, MODEL_NAME, TOKEN, curr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process 수행\n",
    "bert_run.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
